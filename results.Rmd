# Results

```{r}
set.seed(440)
source("gibbs-wrapper.R")
require("plot3D")
# install.packages("plot3D") # sometimes require() does not install the plot3D package, if so, run this line

```

To demonstrate that the gibbs sampler can detect distinguishable clusters, we simulated numerical data such that the clusters are clear separated from each other.


```{r}
#--------------------------------------------
# generate simulated data
# -------------------------------------------
n <- 600
A <- runif(n = 200,min = -1, max = 1)
B <- runif(n = 200, min = 100, max = 110)
C <- runif(n = 200, min = -200, max = -190 )
D <- runif(n = n, min = -5, max = 5)
# note that the data generated and stored in variables A,B,C are clearly in 3 distinct clusters

sim_data <- matrix(nrow = n,ncol = 2)
sim_data[,1] <- c(A,B,C)
sim_data[,2] <- D
var_sim1 <- var(sim_data[,1])
var_sim2 <- var(sim_data[,2])
cov_sim <- cov(sim_data[,1],sim_data[,2])
# inverse of fisher obvs is the asymptotic covariance matrix,
# thus  we will use the inverse of the observed covariance matrix as an estimate for the fisher obvs
sim_obs_info <- solve(matrix(data = c(var_sim1,cov_sim,
                                cov_sim,var_sim2),nrow = 2,ncol=2,byrow=TRUE))


plot(x=sim_data[,1],y=sim_data[,2], main = "simulated data")
```

Now we use the proposed Gibbs Sampler to determine the three clusters in this dataset.

```{r}
# fit gibbs sampler
gibbs_fit2 <- gibbsSampler(data=sim_data, V=sim_obs_info, burnin_period=1e2, numIter=1e3, K=3, Theta.out=FALSE, z.out=FALSE)

# get estimated cluster affiliation from gibbs sampler
fitted_clusters4 <- numeric(length=n)
for (i in 1:600){
  fitted_clusters4[i] <- which.max(gibbs_fit2$Lambda[i, ])
}

# indices
index1 <- 1:n
# error rate
sim_results <- as.data.frame(sim_data)
sim_results$col <- fitted_clusters4
sim_results$col[sim_results$col == 1] <- "red"
sim_results$col[sim_results$col == 2] <- "green"
sim_results$col[sim_results$col == 3] <- "blue"

plot(x=sim_results$V1,y = sim_results$V2,col = sim_results$col, main="clusters determined by gibbs sampler")
```

In reality, it is often difficult to determine if there are distinct clusters in the dataset. If the data has many dimensions, it is very difficult to visually determine if there are distinct clusters, as was the case with the data simulated earlier. 

A real dataset that we will use in this case is dataset used by researchers trying to model microparticle trajectories with a linear drift fractional Brownian motion model (Hill et al., 2016).

This dataset contains the parameter estimates to the trajectories of 668 microparticles, their weight percentage of human bronchial epithelial (hbe) mucosal fluid, and their observed fisher information matrices. 

In this particular dataset, all observations had weight percentage of hbe as 1.5%, 2%, 2.5%, 3%, 4%, 5%.

We will see the gibbs sampler can the group the observations into 6 clusters and see if each cluster corresponds to a particular weight percentage of hbe.

```{r}

hbe_fit <- readRDS("hbe-fbm_fit_v1.rds")
N <- length(hbe_fit)
p <- length(hbe_fit[[1]]$coef)

# Extract data 
wts <- vector(length=N)
theta_hat <- matrix(nrow=N, ncol=p)
obs_info <- array(dim=c(p, p, N))
for (i in 1:N){
  theta_hat[i, ] <- hbe_fit[[i]]$coef
  wts[i] <- hbe_fit[[i]]$wt
  obs_info[, , i] <- hbe_fit[[i]]$vcov
}

# Transform the wts into numeric labels
lab <- unique(wts) 
K <- length(lab)
cluster <- numeric(length=N)
for (i in 1:N){
  if (wts[i] == lab[1]){
    cluster[i] <- 1
  }
  else if (wts[i] == lab[2]){
    cluster[i] <- 2
  }
  else if (wts[i] == lab[3]){
    cluster[i] <- 3
  }
  else if (wts[i] == lab[4]){
    cluster[i] <- 4
  }
  else if (wts[i] == lab[5]){
    cluster[i] <- 5
  }
  else{
    cluster[i] <- 6
  }
}

# getting true cluster affiliations
real_N_k <- numeric(length=K)
real_cols <- rep(NA,N) # colors for true cluster affiliations
for (k in 1:K){
  real_N_k[k] <- sum(cluster == k)
}

# assigning colours to each cluster
real_cols[cluster == 1] = "red"
real_cols[cluster == 2] = "blue"
real_cols[cluster == 3] = "green"
real_cols[cluster == 4] = "orange"
real_cols[cluster == 5] = "black"
real_cols[cluster == 6] = "pink"


pairs(theta_hat,col=real_cols, main="pairs plot") # pairs plot
```


Based on the pairs plot, which produces all the 2-dimensional projections of the data, visually there are no clear clusters since clusters tend to overlap each other. 

Despite this, we will see what clusters are determined by the gibbs sampler, and if they match the clusters determined by hbe weight percentage.


```{r results="hide"}
# fit gibbs sampler
gibbs_fit <- gibbsSampler(data=theta_hat, V=obs_info, burnin_period=1e3, numIter=1e4, K=6, Theta.out=FALSE, z.out=FALSE)
```

```{r}
# get estimated cluster affiliation from gibbs sampler
fitted_clusters <- numeric(length=N)
for (i in 1:N){
  fitted_clusters[i] <- which.max(gibbs_fit$Lambda[i, ])
}

# error rate
err <-  sum(cluster != fitted_clusters)/ N
print(paste0("Percentage of estimated cluster memberships that do not match true cluster memberships: ", round(err,5)))
```

With an error rate of over 95% it is clear that the gibbs sampler did not pick out the same clusters as those determined by the weight percentage. 

As a comparison, we will see if the kmeans plus plus (km++) algorithm performs better. 

```{r}
# clustering real data with kmeans++ algorithm
# note since kmeans starts with random centroids, individual kmeans can have different results.
# We will conduct many kmeans and aggregate the results

#' Initial cluster allocation.
#'
#' Initializes the clusters using the kmeans++ algorithm of Arthur & Vassilvitskii (2007).
#'
#' @param y An `N x p` matrix of observations, each of which is a column.
#' @param K Number of clusters (positive integer).
#' @param max_iter The maximum number of steps in the [stats::kmeans()] algorithm.
#' @param nstart The number of random starts in the [stats::kmeans()] algorithm.
#'
#' @return A vector of length `N` consisting of integers between `1` and `K` specifying an initial clustering of the observations.
#' @author Martin Lysy
#' @references Arthur, D., Vassilvitskii, S. "k-means++: the advantages of careful seeding" *Proceedings of the 18th Annual ACM-SIAM Symposium on Discrete Algorithms.* (2007): 1027â€“1035. <http://ilpubs.stanford.edu:8090/778/1/2006-13.pdf>.
init_z <- function(y, K, max_iter = 10, nstart = 10) {
  # init++
  N <- nrow(y)
  p <- ncol(y)
  x <- t(y) # easier to use columns
  centers <- matrix(NA, p, K) # centers
  icenters <- rep(NA, K-1) # indices of centers
  minD <- rep(Inf, N) # current minimum distance vector
  # initialize
  icenters[1] <- sample(N,1)
  centers[,1] <- x[,icenters[1]]
  for(ii in 1:(K-1)) {
    D <- colSums((x - centers[,ii])^2) # new distance
    minD <- pmin(minD, D)
    icenters[ii+1] <- sample(N, 1, prob = minD)
    centers[,ii+1] <- x[,icenters[ii+1]]
  }
  centers <- t(centers)
  colnames(centers) <- rownames(x)
  # run kmeans with these centers
  km <- kmeans(x = y, centers = centers, nstart = nstart, iter.max = max_iter)
  km$cluster
}

max_iters <- 10000  # set number of kmeans to run
kmeans_results <- matrix(nrow = N,ncol = max_iters) # pre-allocate matrix

# simple function to calculate the mode in vector of numbers
Mode <- function(x) {
  ux <- unique(x)
  ux[which.max(tabulate(match(x, ux)))]
}

# get results from max_iters kmeans
for (iter in 1:max_iters){
  kmeans_results[,iter] <- init_z(y = theta_hat,K = 6)
}

fitted_clusters1 <- matrix(nrow = N,ncol = 1) # pre-allocate matrix to store estimated clusters

# the mode cluster membership for each observation becomes the estimate cluster membership
for (row in 1:N){
  fitted_clusters1[row,] <- Mode(kmeans_results[row,])
}

err1 <- sum(cluster != fitted_clusters1) / N
print(paste0("Percentage of estimated cluster memberships that do not match true cluster memberships: ", round(err1,5)))
```

While the km++ algorithm performed better than the gibbs sampler, it still had an error rate of 70%, which means that it also did not pick out the same clusters as those determined by the weight percentage. 

It is possible that this is caused by the data being unscaled. Thus, we carry out the same test but the data has been standardized so that mean of the data is 0 and the variance is 1.

```{r}
#-----------------------------------------------------------------------------------------------------------
# testing with standardized data (data centered at 0 with sd of 1)
#-----------------------------------------------------------------------------------------------------------
theta_hat_scaled <- scale(theta_hat)

# fit gibbs sampler
gibbs_fit1 <- gibbsSampler(data=theta_hat_scaled, V=obs_info, burnin_period=1e3, numIter=1e4, K=6, Theta.out=FALSE, z.out=FALSE)
```

```{r}
# get estimated cluster affiliation from gibbs sampler
fitted_clusters2 <- numeric(length=N)
for (i in 1:N){
  fitted_clusters2[i] <- which.max(gibbs_fit1$Lambda[i, ])
}

# error rate
err2 <- sum(cluster != fitted_clusters2) / N
print(paste0("Percentage of estimated cluster memberships that do not match true cluster memberships: ",round(err2,5)))
```

The performance of the gibbs sampler improved with the error rate decreasing to 77% but is still poor.

```{r}
max_iters <- 10000
kmeans_results1 <- matrix(nrow = N,ncol = max_iters)

# get results from max_iters kmeans
for (iter in 1:max_iters){
  kmeans_results1[,iter] <- init_z(y = theta_hat_scaled,K = 6)
}

fitted_clusters3 <- matrix(nrow = N,ncol = 1) # pre-allocate matrix

# the mode cluster membership for each observation becomes the estimate cluster membership
for (row in 1:N){
  fitted_clusters3[row,] <- Mode(kmeans_results1[row,])
}

err3 <- sum(cluster != fitted_clusters3) / N
print(paste0("Percentage of estimated cluster memberships that do not match true cluster memberships: ",round(err3,5)))

```

The km++ algorithm perform even worse with the error rate increasing to 82%.

Even with scaled data, both clustering algorithms did not implicitly distinguished observations by hbe weight percentage.


Here are the clusters determined the gibbs sampler on scaled data.

```{r}
gibb_cols <- fitted_clusters2
# assigning colours to each cluster
gibb_cols[gibb_cols == 1] = "red"
gibb_cols[gibb_cols == 2] = "blue"
gibb_cols[gibb_cols == 3] = "green"
gibb_cols[gibb_cols == 4] = "orange"
gibb_cols[gibb_cols == 5] = "black"
gibb_cols[gibb_cols == 6] = "pink"

pairs(theta_hat,col=gibb_cols, main="pairs plot") # pairs plot
```

As expected, the clusters determined by the gibbs sampler overlap each other heavily.

It is difficult to assess if the gibbs sampler or the km++ fit the data well, because there is no true cluster affiliation to evaluate against. Additionally, the data with the given variables cannot clearly by group up into distinguishable groups.  

Overall, this suggests that given the existing parameters, observations with particular hbe weight percentages are not very distinguishable from other observations with different hbe weight percentages.

This does not rule out the possibility that hbe weight percentages or other variables can be used to group up microparticle trajectories, but for this to occur, latent variables would need be made known.




